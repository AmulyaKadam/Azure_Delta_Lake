# ğŸš€ Azure Delta Lake Project (Databricks + ADF)

This project demonstrates an **end-to-end modern data engineering architecture** on **Microsoft Azure** using **Azure Data Factory (ADF)**, **Azure Databricks (Apache Spark)**, and **Delta Lake**. The solution follows the **Medallion Architecture (Bronze, Silver, Gold)** to ingest, transform, and serve data for analytics and reporting.

---

## ğŸ“˜ Table of Contents

- [Detailed Overview](#-detailed-overview)
- [ğŸš€ Project Requirements](#-project-requirements)
- [Architecture](#-architecture)
- [Data Flow (Data Lineage)](#-data-flow-data-lineage)
- [ADF Pipeline Execution & Monitoring](#-adf-pipeline-execution--monitoring)
- [Technologies Used](#-technologies-used)
- [Folder Structure](#-folder-structure)
- [How to Use](#-how-to-use)
- [Future Enhancements](#-future-enhancements)
- [Contact](#-contact)

---

## ğŸ§© Detailed Overview

The **Azure Delta Lake Project** showcases how to design and implement a **scalable, cloud-native data platform** using Azure services.

The primary objective is to **ingest, process, and transform data** from multiple source systems (ERP and CRM) into a **Delta Lake-based analytical data store** that supports BI, ad-hoc analytics, and machine learning workloads.

This project highlights:

- **Azure Data Factory (ADF)** for data ingestion and orchestration
- **Azure Databricks (Apache Spark)** for distributed data processing
- **Delta Lake** for reliable, ACID-compliant data storage
- **Medallion Architecture** (Bronze â†’ Silver â†’ Gold) for structured data refinement

The repository serves as a practical reference for **data engineers and cloud practitioners** looking to build production-grade data pipelines on Azure.

---

## ğŸš€ Project Requirements

### ğŸ§  Objective

Build a **modern Azure-based data platform** to consolidate sales data and enable analytical reporting using **Databricks and Delta Lake**.

### ğŸ“‹ Specifications

- **Data Sources:** ERP and CRM datasets (CSV files)
- **Ingestion:** Orchestrated using **Azure Data Factory pipelines**
- **Processing:** Data transformation using **PySpark / Spark SQL** in Azure Databricks
- **Storage:** Delta Lake tables stored in **Azure Data Lake Storage Gen2**
- **Architecture:** Bronze, Silver, and Gold layers
- **Scope:** Latest snapshot data (no historization)
- **Documentation:** Clear architecture, data flow, and transformation logic

---

## ğŸ—ï¸ Architecture

![Azure Data Architecture](./Images/Azure_Data_architecture.drawio.png)

**High-Level Architecture Flow:**

1. Source data (ERP & CRM CSV files)
2. Azure Data Factory for ingestion and orchestration
3. Azure Data Lake Storage Gen2 as the storage layer
4. Azure Databricks for Spark-based transformations
5. Delta Lake tables organized into Bronze, Silver, and Gold layers
6. Consumption by Power BI / Analytics tools

---

## ğŸ”„ Data Flow (Data Lineage)

![Data Flow Diagram](./Images/Flow_diagram.drawio.png)

---

## ğŸ“Š ADF Pipeline Execution & Monitoring

![ADF Pipeline Log](./Images/ADF_pipeline_logs.PNG)

ADF pipelines are used to:

- Trigger data ingestion from source systems
- Execute Databricks notebooks
- Monitor pipeline execution and failures

---

## âš™ï¸ Technologies Used

| Category | Tools / Technologies |
|--------|---------------------|
| **Cloud Platform** | Microsoft Azure |
| **Orchestration** | Azure Data Factory (ADF) |
| **Processing Engine** | Azure Databricks (Apache Spark) |
| **Storage** | Azure Data Lake Storage Gen2 |
| **Data Format** | Delta Lake |
| **Transformation** | PySpark / Spark SQL |
| **Visualization** | Power BI |
| **Design & Diagrams** | draw.io |
| **Source Systems** | ERP & CRM CSV files |

---

## ğŸ“ Folder Structure

```
Azure_Delta_Lake_Project/
â”‚
â”œâ”€â”€ data/                         # Source data (ERP, CRM)
â”‚   â”œâ”€â”€ crm/
â”‚   â””â”€â”€ erp/
â”‚
â”œâ”€â”€ adf/                          # Azure Data Factory pipeline JSONs
â”‚   â”œâ”€â”€ ingestion_pipeline.json
â”‚   â””â”€â”€ databricks_trigger.json
â”‚
â”œâ”€â”€ databricks/                   # Databricks notebooks
â”‚   â”œâ”€â”€ bronze_layer/
â”‚   â”œâ”€â”€ silver_layer/
â”‚   â””â”€â”€ gold_layer/
â”‚
â”œâ”€â”€ images/                       # Architecture & pipeline screenshots
â”‚   â”œâ”€â”€ Azure_data_architecture.png
â”‚   â”œâ”€â”€ dataflow.png
â”‚   â””â”€â”€ ADF_pipeline_log.png
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ data_catalog.md
â”‚   â”œâ”€â”€ naming_conventions.md
â”‚   â””â”€â”€ sales_data_model.md
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ CONTRIBUTING.md
```

---

## ğŸš€ How to Use

1. **Clone the repository**

```bash
git clone https://github.com/AmulyaKadam/Azure_Delta_Lake.git
cd Azure_Delta_Lake
```

2. **Set up Azure Resources**

- Azure Data Lake Storage Gen2
- Azure Databricks Workspace
- Azure Data Factory

3. **Upload Source Data**

- Place ERP and CRM CSV files in ADLS Gen2 (raw container)

4. **Run ADF Pipelines**

- Trigger ingestion pipelines
- Monitor execution using ADF pipeline logs

5. **Execute Databricks Notebooks**

- Bronze â†’ Silver â†’ Gold transformations
- Validate Delta tables

6. **Consume Data**

- Connect Power BI to Gold Delta tables
- Run analytical queries or ML workloads

---

## ğŸ”® Future Enhancements

- Implement **incremental data loads** using watermarking
- Add **Delta Live Tables (DLT)**
- Enable **Unity Catalog** for governance
- Implement **data quality checks** using Spark expectations
- Add **CI/CD for ADF & Databricks** using Azure DevOps

---

## ğŸ‘¤ Contact

**Author:** Amulya Kadam  
ğŸ“§ **Email:** [kadamamulya017@gmail.com](mailto:kadamamulya017@gmail.com)  
ğŸ’¼ **GitHub:** AmulyaKadam  
ğŸ”— **LinkedIn:** [www.linkedin.com/in/amulya-kadam-8b3647208](http://www.linkedin.com/in/amulya-kadam-8b3647208)

---

â­ *If you found this project helpful, please star the repository!*

